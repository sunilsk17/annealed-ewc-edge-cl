/Users/sunilkumars/Desktop/EWC Project/drift_cl_edge/venv/lib/python3.13/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)
  entry = pickle.load(f, encoding="latin1")
/Users/sunilkumars/Desktop/EWC Project/drift_cl_edge/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.
  warnings.warn(warn_msg)
Using device: mps
Training MobileNetV3 on Split-CIFAR-100 with LwF
λ_distill=1.0, Temperature=2.0
Split-CIFAR-100 initialized:
  Tasks: 10
  Classes per task: 10
  Class order: [83 53 70 45 44 39 22 80 10  0 18 30 73 33 90  4 76 77 12 31]... (first 20)
Task 0: Classes [83, 53, 70, 45, 44, 39, 22, 80, 10, 0]
  Train samples: 5000
  Test samples: 1000
Task 1: Classes [18, 30, 73, 33, 90, 4, 76, 77, 12, 31]
  Train samples: 5000
  Test samples: 1000
Task 2: Classes [55, 88, 26, 42, 69, 15, 40, 96, 9, 72]
  Train samples: 5000
  Test samples: 1000
Task 3: Classes [11, 47, 85, 28, 93, 5, 66, 65, 35, 16]
  Train samples: 5000
  Test samples: 1000
Task 4: Classes [49, 34, 7, 95, 27, 19, 81, 25, 62, 13]
  Train samples: 5000
  Test samples: 1000
Task 5: Classes [24, 3, 17, 38, 8, 78, 6, 64, 36, 89]
  Train samples: 5000
  Test samples: 1000
Task 6: Classes [56, 99, 54, 43, 50, 67, 46, 68, 61, 97]
  Train samples: 5000
  Test samples: 1000
Task 7: Classes [79, 41, 58, 48, 98, 57, 75, 32, 94, 59]
  Train samples: 5000
  Test samples: 1000
Task 8: Classes [63, 84, 37, 29, 1, 52, 21, 2, 23, 87]
  Train samples: 5000
  Test samples: 1000
Task 9: Classes [91, 74, 86, 82, 20, 60, 71, 14, 92, 51]
  Train samples: 5000
  Test samples: 1000

============================================================
Training on Task 0 (Classes: [83, 53, 70, 45, 44, 39, 22, 80, 10, 0])
============================================================
Epoch 1/10 - Loss: 3.5350 (CE: 3.5350, Distill: 0.0000) - Acc: 0.2396
Epoch 2/10 - Loss: 1.7595 (CE: 1.7595, Distill: 0.0000) - Acc: 0.3920
Epoch 3/10 - Loss: 1.4999 (CE: 1.4999, Distill: 0.0000) - Acc: 0.4604
Epoch 4/10 - Loss: 1.3456 (CE: 1.3456, Distill: 0.0000) - Acc: 0.5266
Epoch 5/10 - Loss: 1.2342 (CE: 1.2342, Distill: 0.0000) - Acc: 0.5630
Epoch 6/10 - Loss: 1.1676 (CE: 1.1676, Distill: 0.0000) - Acc: 0.5856
Epoch 7/10 - Loss: 1.0672 (CE: 1.0672, Distill: 0.0000) - Acc: 0.6228
Epoch 8/10 - Loss: 1.0158 (CE: 1.0158, Distill: 0.0000) - Acc: 0.6382
Epoch 9/10 - Loss: 0.9339 (CE: 0.9339, Distill: 0.0000) - Acc: 0.6618
Epoch 10/10 - Loss: 0.9101 (CE: 0.9101, Distill: 0.0000) - Acc: 0.6754
Saved: checkpoints_lwf_test/model_task0.pt
Teacher model updated (frozen)

============================================================
Training on Task 1 (Classes: [18, 30, 73, 33, 90, 4, 76, 77, 12, 31])
Using distillation on 10 old classes
============================================================
Epoch 1/10 - Loss: 5.6057 (CE: 4.6862, Distill: 0.9195) - Acc: 0.1028
Epoch 2/10 - Loss: 2.3252 (CE: 1.9290, Distill: 0.3962) - Acc: 0.4508
Epoch 3/10 - Loss: 2.0275 (CE: 1.6466, Distill: 0.3809) - Acc: 0.5042
Epoch 4/10 - Loss: 1.8749 (CE: 1.5067, Distill: 0.3682) - Acc: 0.5374
Epoch 5/10 - Loss: 1.8039 (CE: 1.4429, Distill: 0.3610) - Acc: 0.5692
Epoch 6/10 - Loss: 1.7100 (CE: 1.3364, Distill: 0.3736) - Acc: 0.5846
Epoch 7/10 - Loss: 1.6303 (CE: 1.2690, Distill: 0.3613) - Acc: 0.5974
Epoch 8/10 - Loss: 1.5676 (CE: 1.2104, Distill: 0.3571) - Acc: 0.6194
Epoch 9/10 - Loss: 1.4802 (CE: 1.1268, Distill: 0.3534) - Acc: 0.6264
Epoch 10/10 - Loss: 1.3912 (CE: 1.0613, Distill: 0.3299) - Acc: 0.6478
Saved: checkpoints_lwf_test/model_task1.pt
Teacher model updated (frozen)

============================================================
Training on Task 2 (Classes: [55, 88, 26, 42, 69, 15, 40, 96, 9, 72])
Using distillation on 20 old classes
============================================================
Epoch 1/10 - Loss: 6.5505 (CE: 5.3558, Distill: 1.1947) - Acc: 0.0578
Epoch 2/10 - Loss: 2.8005 (CE: 2.1513, Distill: 0.6492) - Acc: 0.3794
Epoch 3/10 - Loss: 2.4949 (CE: 1.8943, Distill: 0.6006) - Acc: 0.4276
Epoch 4/10 - Loss: 2.3448 (CE: 1.7450, Distill: 0.5997) - Acc: 0.4760
Epoch 5/10 - Loss: 2.2343 (CE: 1.6744, Distill: 0.5599) - Acc: 0.4908
Epoch 6/10 - Loss: 2.1926 (CE: 1.6354, Distill: 0.5572) - Acc: 0.5118
Epoch 7/10 - Loss: 2.0613 (CE: 1.4991, Distill: 0.5622) - Acc: 0.5284
Epoch 8/10 - Loss: 2.0314 (CE: 1.4624, Distill: 0.5690) - Acc: 0.5454
Epoch 9/10 - Loss: 1.9685 (CE: 1.3954, Distill: 0.5731) - Acc: 0.5482
Epoch 10/10 - Loss: 1.8878 (CE: 1.3440, Distill: 0.5438) - Acc: 0.5636
Saved: checkpoints_lwf_test/model_task2.pt
Teacher model updated (frozen)

============================================================
Training on Task 3 (Classes: [11, 47, 85, 28, 93, 5, 66, 65, 35, 16])
Using distillation on 30 old classes
============================================================
Epoch 1/10 - Loss: 7.3601 (CE: 6.0383, Distill: 1.3218) - Acc: 0.0184
Epoch 2/10 - Loss: 3.3552 (CE: 2.3590, Distill: 0.9962) - Acc: 0.3624
Epoch 3/10 - Loss: 2.9758 (CE: 2.0557, Distill: 0.9200) - Acc: 0.4408
Epoch 4/10 - Loss: 2.8718 (CE: 1.9724, Distill: 0.8994) - Acc: 0.4712
Epoch 5/10 - Loss: 2.7121 (CE: 1.8060, Distill: 0.9061) - Acc: 0.4784
Epoch 6/10 - Loss: 2.6109 (CE: 1.7314, Distill: 0.8795) - Acc: 0.5172
Epoch 7/10 - Loss: 2.5442 (CE: 1.6451, Distill: 0.8991) - Acc: 0.5194
Epoch 8/10 - Loss: 2.4703 (CE: 1.5718, Distill: 0.8985) - Acc: 0.5340
Epoch 9/10 - Loss: 2.3970 (CE: 1.5208, Distill: 0.8762) - Acc: 0.5518
Epoch 10/10 - Loss: 2.2868 (CE: 1.4123, Distill: 0.8746) - Acc: 0.5802
Saved: checkpoints_lwf_test/model_task3.pt
Teacher model updated (frozen)

============================================================
Training on Task 4 (Classes: [49, 34, 7, 95, 27, 19, 81, 25, 62, 13])
Using distillation on 40 old classes
============================================================
Epoch 1/10 - Loss: 6.9699 (CE: 5.5142, Distill: 1.4557) - Acc: 0.0966
Epoch 2/10 - Loss: 3.3658 (CE: 2.0954, Distill: 1.2704) - Acc: 0.4138
Epoch 3/10 - Loss: 3.0170 (CE: 1.8176, Distill: 1.1994) - Acc: 0.4780
Epoch 4/10 - Loss: 2.8945 (CE: 1.7458, Distill: 1.1487) - Acc: 0.4974
Epoch 5/10 - Loss: 2.7751 (CE: 1.6197, Distill: 1.1554) - Acc: 0.5320
Epoch 6/10 - Loss: 2.6870 (CE: 1.5555, Distill: 1.1315) - Acc: 0.5488
Epoch 7/10 - Loss: 2.6356 (CE: 1.5082, Distill: 1.1274) - Acc: 0.5706
Epoch 8/10 - Loss: 2.5794 (CE: 1.4705, Distill: 1.1089) - Acc: 0.5798
Epoch 9/10 - Loss: 2.5040 (CE: 1.3861, Distill: 1.1179) - Acc: 0.5936
Epoch 10/10 - Loss: 2.4757 (CE: 1.3458, Distill: 1.1299) - Acc: 0.6044
Saved: checkpoints_lwf_test/model_task4.pt
Teacher model updated (frozen)

============================================================
Training on Task 5 (Classes: [24, 3, 17, 38, 8, 78, 6, 64, 36, 89])
Using distillation on 50 old classes
============================================================
Epoch 1/10 - Loss: 6.6845 (CE: 5.2456, Distill: 1.4389) - Acc: 0.0930
Epoch 2/10 - Loss: 3.5983 (CE: 2.2420, Distill: 1.3563) - Acc: 0.3752
Epoch 3/10 - Loss: 3.2579 (CE: 2.0068, Distill: 1.2510) - Acc: 0.4582
Epoch 4/10 - Loss: 3.1428 (CE: 1.9343, Distill: 1.2085) - Acc: 0.4934
Epoch 5/10 - Loss: 3.0361 (CE: 1.8337, Distill: 1.2024) - Acc: 0.5306
Epoch 6/10 - Loss: 2.9265 (CE: 1.7382, Distill: 1.1883) - Acc: 0.5542
Epoch 7/10 - Loss: 2.8878 (CE: 1.7027, Distill: 1.1851) - Acc: 0.5832
Epoch 8/10 - Loss: 2.7634 (CE: 1.5821, Distill: 1.1813) - Acc: 0.6000
Epoch 9/10 - Loss: 2.7175 (CE: 1.5454, Distill: 1.1722) - Acc: 0.6192
Epoch 10/10 - Loss: 2.6358 (CE: 1.4771, Distill: 1.1587) - Acc: 0.6364
Saved: checkpoints_lwf_test/model_task5.pt
Teacher model updated (frozen)

============================================================
Training on Task 6 (Classes: [56, 99, 54, 43, 50, 67, 46, 68, 61, 97])
Using distillation on 60 old classes
============================================================
Epoch 1/10 - Loss: 6.7961 (CE: 5.4502, Distill: 1.3460) - Acc: 0.1320
Epoch 2/10 - Loss: 2.9882 (CE: 1.8888, Distill: 1.0994) - Acc: 0.5156
Epoch 3/10 - Loss: 2.5598 (CE: 1.5755, Distill: 0.9843) - Acc: 0.6016
Epoch 4/10 - Loss: 2.3995 (CE: 1.4659, Distill: 0.9336) - Acc: 0.6406
Epoch 5/10 - Loss: 2.2640 (CE: 1.3448, Distill: 0.9192) - Acc: 0.6704
Epoch 6/10 - Loss: 2.1921 (CE: 1.2885, Distill: 0.9036) - Acc: 0.6994
Epoch 7/10 - Loss: 2.1392 (CE: 1.2321, Distill: 0.9071) - Acc: 0.7136
Epoch 8/10 - Loss: 2.0745 (CE: 1.1696, Distill: 0.9049) - Acc: 0.7336
Epoch 9/10 - Loss: 1.9781 (CE: 1.0819, Distill: 0.8961) - Acc: 0.7540
Epoch 10/10 - Loss: 1.9146 (CE: 1.0315, Distill: 0.8830) - Acc: 0.7632
Saved: checkpoints_lwf_test/model_task6.pt
Teacher model updated (frozen)

============================================================
Training on Task 7 (Classes: [79, 41, 58, 48, 98, 57, 75, 32, 94, 59])
Using distillation on 70 old classes
============================================================
Epoch 1/10 - Loss: 6.7983 (CE: 5.2664, Distill: 1.5319) - Acc: 0.1762
Epoch 2/10 - Loss: 2.9997 (CE: 1.7442, Distill: 1.2555) - Acc: 0.5596
Epoch 3/10 - Loss: 2.5629 (CE: 1.4863, Distill: 1.0766) - Acc: 0.6302
Epoch 4/10 - Loss: 2.4243 (CE: 1.3905, Distill: 1.0338) - Acc: 0.6624
Epoch 5/10 - Loss: 2.2823 (CE: 1.2736, Distill: 1.0087) - Acc: 0.7010
Epoch 6/10 - Loss: 2.1908 (CE: 1.2012, Distill: 0.9896) - Acc: 0.7150
Epoch 7/10 - Loss: 2.1802 (CE: 1.1760, Distill: 1.0042) - Acc: 0.7326
Epoch 8/10 - Loss: 2.0564 (CE: 1.0677, Distill: 0.9886) - Acc: 0.7518
Epoch 9/10 - Loss: 2.0279 (CE: 1.0578, Distill: 0.9701) - Acc: 0.7672
Epoch 10/10 - Loss: 1.9782 (CE: 0.9999, Distill: 0.9783) - Acc: 0.7718
Saved: checkpoints_lwf_test/model_task7.pt
Teacher model updated (frozen)

============================================================
Training on Task 8 (Classes: [63, 84, 37, 29, 1, 52, 21, 2, 23, 87])
Using distillation on 80 old classes
============================================================
Epoch 1/10 - Loss: 6.4000 (CE: 5.1172, Distill: 1.2828) - Acc: 0.1758
Epoch 2/10 - Loss: 3.0475 (CE: 1.6784, Distill: 1.3691) - Acc: 0.5698
Epoch 3/10 - Loss: 2.6454 (CE: 1.4651, Distill: 1.1803) - Acc: 0.6358
Epoch 4/10 - Loss: 2.4990 (CE: 1.3495, Distill: 1.1495) - Acc: 0.6834
Epoch 5/10 - Loss: 2.4490 (CE: 1.3058, Distill: 1.1432) - Acc: 0.7050
Epoch 6/10 - Loss: 2.3671 (CE: 1.2311, Distill: 1.1359) - Acc: 0.7254
Epoch 7/10 - Loss: 2.3063 (CE: 1.1732, Distill: 1.1332) - Acc: 0.7426
Epoch 8/10 - Loss: 2.2847 (CE: 1.1626, Distill: 1.1221) - Acc: 0.7588
Epoch 9/10 - Loss: 2.2146 (CE: 1.0969, Distill: 1.1177) - Acc: 0.7636
Epoch 10/10 - Loss: 2.1713 (CE: 1.0563, Distill: 1.1150) - Acc: 0.7804
Saved: checkpoints_lwf_test/model_task8.pt
Teacher model updated (frozen)

============================================================
Training on Task 9 (Classes: [91, 74, 86, 82, 20, 60, 71, 14, 92, 51])
Using distillation on 90 old classes
============================================================
Epoch 1/10 - Loss: 6.4188 (CE: 4.8397, Distill: 1.5791) - Acc: 0.2006
Epoch 2/10 - Loss: 3.1877 (CE: 1.6757, Distill: 1.5120) - Acc: 0.5544
Epoch 3/10 - Loss: 2.7050 (CE: 1.4342, Distill: 1.2707) - Acc: 0.6320
Epoch 4/10 - Loss: 2.6592 (CE: 1.3941, Distill: 1.2651) - Acc: 0.6728
Epoch 5/10 - Loss: 2.4815 (CE: 1.2624, Distill: 1.2192) - Acc: 0.6944
Epoch 6/10 - Loss: 2.4060 (CE: 1.2211, Distill: 1.1849) - Acc: 0.7062
Epoch 7/10 - Loss: 2.3514 (CE: 1.1679, Distill: 1.1835) - Acc: 0.7268
Epoch 8/10 - Loss: 2.2977 (CE: 1.1197, Distill: 1.1779) - Acc: 0.7432
Epoch 9/10 - Loss: 2.2330 (CE: 1.0670, Distill: 1.1659) - Acc: 0.7596
Epoch 10/10 - Loss: 2.1730 (CE: 1.0211, Distill: 1.1519) - Acc: 0.7770
Saved: checkpoints_lwf_test/model_task9.pt
Teacher model updated (frozen)

============================================================
LwF Training Complete!
λ_distill=1.0, Temperature=2.0
============================================================
